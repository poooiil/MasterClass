{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b29249-26fa-45ed-be3c-e9cbd9844ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c012c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_tensor = torch.tensor(np.load(\"train_data.npy\"))\n",
    "#切分训练/测试集\n",
    "train_tensor = pose_tensor[:400]\n",
    "test_tensor = pose_tensor[400:]\n",
    "\n",
    "x1, x2, x3 = torch.chunk(train_tensor, chunks=3, dim=2)\n",
    "x1 = x1.permute(2,0,3,1)[0]\n",
    "D1 = x1[:,0,:9]\n",
    "motion1 = x1[:,1:].reshape(-1,120,23,6)\n",
    "\n",
    "x2 = x2.permute(2,0,3,1)[0]\n",
    "D2 = x2[:,0,:9]\n",
    "motion2 = x2[:,1:].reshape(-1,120,23,6)\n",
    "\n",
    "x3 = x3.permute(2,0,3,1)[0]\n",
    "cam = x3[:,1:,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f9db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#38, 3, 121\n",
    "#转换成标准的 PyTorch DataLoader 格式\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomMotionDataset(Dataset):\n",
    "    def __init__(self, seq1, seq2, vec1, vec2, labels):\n",
    "        self.seq1 = seq1.to(torch.float32)\n",
    "        self.seq2 = seq2.to(torch.float32)\n",
    "        self.vec1 = vec1.to(torch.float32)\n",
    "        self.vec2 = vec2.to(torch.float32)\n",
    "        self.labels = labels.to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"seq1\": self.seq1[idx],\n",
    "            \"seq2\": self.seq2[idx],\n",
    "            \"vec1\": self.vec1[idx],\n",
    "            \"vec2\": self.vec2[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "dataset = CustomMotionDataset(motion1, motion2, D1, D2, cam)\n",
    "\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3865b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build transformer，输入双人动作，输出时序编码，学习映射\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim=512, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=512, dropout=0.1, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.output_layer = nn.Linear(embed_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class VectorProcessor(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "#输出 Toric 摄像机参数\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.series_model = TimeSeriesTransformer(input_dim=6*23, output_dim=512)\n",
    "        self.vector_model = VectorProcessor(input_dim=9, embed_dim=512)\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(512 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 6)\n",
    "        )\n",
    "    \n",
    "    def forward(self, seq1, seq2, vec1, vec2):\n",
    "        batch_size = seq1.shape[0]\n",
    "        seq1 = seq1.view(batch_size, 120, -1)  # Reshape (120, 23, 6) -> (120, 23*6)\n",
    "        seq2 = seq2.view(batch_size, 120, -1)\n",
    "        \n",
    "        seq1_feat = self.series_model(seq1)\n",
    "        seq2_feat = self.series_model(seq2)\n",
    "        vec1_feat = self.vector_model(vec1).unsqueeze(1).repeat(1, 120, 1)  # Expand to match time dimension\n",
    "        vec2_feat = self.vector_model(vec2).unsqueeze(1).repeat(1, 120, 1)\n",
    "        \n",
    "        fused = torch.cat([seq1_feat, seq2_feat, vec1_feat, vec2_feat], dim=-1) #输出的6维对应论文中定义的 Toric 特征 (L2Dx, L2Dy, R2Dx, R2Dy, Theta, Phi)，即镜头左/右人物在屏幕上的归一化位置加开合角、仰俯角等。\n",
    "        output = self.fusion_fc(fused)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3767bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionModel().to(\"cuda\")\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d8874c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1.0676\n",
      "Epoch [2/1000], Loss: 1.1067\n",
      "Epoch [3/1000], Loss: 0.8388\n",
      "Epoch [4/1000], Loss: 0.8743\n",
      "Epoch [5/1000], Loss: 0.9676\n",
      "Epoch [6/1000], Loss: 0.8351\n",
      "Epoch [7/1000], Loss: 1.1806\n",
      "Epoch [8/1000], Loss: 0.7693\n",
      "Epoch [9/1000], Loss: 0.5348\n",
      "Epoch [10/1000], Loss: 0.6891\n",
      "Epoch [11/1000], Loss: 0.4762\n",
      "Epoch [12/1000], Loss: 0.6074\n",
      "Epoch [13/1000], Loss: 0.6333\n",
      "Epoch [14/1000], Loss: 0.5520\n",
      "Epoch [15/1000], Loss: 0.4761\n",
      "Epoch [16/1000], Loss: 0.5094\n",
      "Epoch [17/1000], Loss: 0.4549\n",
      "Epoch [18/1000], Loss: 0.8415\n",
      "Epoch [19/1000], Loss: 0.4172\n",
      "Epoch [20/1000], Loss: 0.4136\n",
      "Epoch [21/1000], Loss: 0.5024\n",
      "Epoch [22/1000], Loss: 0.7283\n",
      "Epoch [23/1000], Loss: 0.4468\n",
      "Epoch [24/1000], Loss: 0.3606\n",
      "Epoch [25/1000], Loss: 0.4811\n",
      "Epoch [26/1000], Loss: 0.5387\n",
      "Epoch [27/1000], Loss: 0.6752\n",
      "Epoch [28/1000], Loss: 0.3941\n",
      "Epoch [29/1000], Loss: 0.5264\n",
      "Epoch [30/1000], Loss: 0.4792\n",
      "Epoch [31/1000], Loss: 0.3775\n",
      "Epoch [32/1000], Loss: 0.5860\n",
      "Epoch [33/1000], Loss: 0.2955\n",
      "Epoch [34/1000], Loss: 0.4001\n",
      "Epoch [35/1000], Loss: 0.1815\n",
      "Epoch [36/1000], Loss: 0.3856\n",
      "Epoch [37/1000], Loss: 0.4174\n",
      "Epoch [38/1000], Loss: 0.5616\n",
      "Epoch [39/1000], Loss: 0.4273\n",
      "Epoch [40/1000], Loss: 0.3521\n",
      "Epoch [41/1000], Loss: 0.4462\n",
      "Epoch [42/1000], Loss: 0.4397\n",
      "Epoch [43/1000], Loss: 0.3564\n",
      "Epoch [44/1000], Loss: 0.2938\n",
      "Epoch [45/1000], Loss: 0.3081\n",
      "Epoch [46/1000], Loss: 0.3558\n",
      "Epoch [47/1000], Loss: 0.4584\n",
      "Epoch [48/1000], Loss: 0.5682\n",
      "Epoch [49/1000], Loss: 0.3787\n",
      "Epoch [50/1000], Loss: 0.3676\n",
      "Epoch [51/1000], Loss: 0.2943\n",
      "Epoch [52/1000], Loss: 0.2898\n",
      "Epoch [53/1000], Loss: 0.3466\n",
      "Epoch [54/1000], Loss: 0.3388\n",
      "Epoch [55/1000], Loss: 0.3204\n",
      "Epoch [56/1000], Loss: 0.4415\n",
      "Epoch [57/1000], Loss: 0.3363\n",
      "Epoch [58/1000], Loss: 0.2915\n",
      "Epoch [59/1000], Loss: 0.3902\n",
      "Epoch [60/1000], Loss: 0.3811\n",
      "Epoch [61/1000], Loss: 0.2796\n",
      "Epoch [62/1000], Loss: 0.3718\n",
      "Epoch [63/1000], Loss: 0.1877\n",
      "Epoch [64/1000], Loss: 0.3167\n",
      "Epoch [65/1000], Loss: 0.3853\n",
      "Epoch [66/1000], Loss: 0.2595\n",
      "Epoch [67/1000], Loss: 0.2977\n",
      "Epoch [68/1000], Loss: 0.3101\n",
      "Epoch [69/1000], Loss: 0.3424\n",
      "Epoch [70/1000], Loss: 0.3320\n",
      "Epoch [71/1000], Loss: 0.2289\n",
      "Epoch [72/1000], Loss: 0.3821\n",
      "Epoch [73/1000], Loss: 0.1900\n",
      "Epoch [74/1000], Loss: 0.2299\n",
      "Epoch [75/1000], Loss: 0.2675\n",
      "Epoch [76/1000], Loss: 0.3671\n",
      "Epoch [77/1000], Loss: 0.2135\n",
      "Epoch [78/1000], Loss: 0.3015\n",
      "Epoch [79/1000], Loss: 0.3295\n",
      "Epoch [80/1000], Loss: 0.2722\n",
      "Epoch [81/1000], Loss: 0.2570\n",
      "Epoch [82/1000], Loss: 0.3143\n",
      "Epoch [83/1000], Loss: 0.2607\n",
      "Epoch [84/1000], Loss: 0.2548\n",
      "Epoch [85/1000], Loss: 0.2176\n",
      "Epoch [86/1000], Loss: 0.2602\n",
      "Epoch [87/1000], Loss: 0.1817\n",
      "Epoch [88/1000], Loss: 0.1796\n",
      "Epoch [89/1000], Loss: 0.3002\n",
      "Epoch [90/1000], Loss: 0.2681\n",
      "Epoch [91/1000], Loss: 0.3116\n",
      "Epoch [92/1000], Loss: 0.2848\n",
      "Epoch [93/1000], Loss: 0.2737\n",
      "Epoch [94/1000], Loss: 0.1890\n",
      "Epoch [95/1000], Loss: 0.2807\n",
      "Epoch [96/1000], Loss: 0.1784\n",
      "Epoch [97/1000], Loss: 0.2261\n",
      "Epoch [98/1000], Loss: 0.2949\n",
      "Epoch [99/1000], Loss: 0.2482\n",
      "Epoch [100/1000], Loss: 0.2416\n",
      "Epoch [101/1000], Loss: 0.2380\n",
      "Epoch [102/1000], Loss: 0.2985\n",
      "Epoch [103/1000], Loss: 0.1902\n",
      "Epoch [104/1000], Loss: 0.2102\n",
      "Epoch [105/1000], Loss: 0.2377\n",
      "Epoch [106/1000], Loss: 0.1669\n",
      "Epoch [107/1000], Loss: 0.1975\n",
      "Epoch [108/1000], Loss: 0.2543\n",
      "Epoch [109/1000], Loss: 0.2288\n",
      "Epoch [110/1000], Loss: 0.1542\n",
      "Epoch [111/1000], Loss: 0.2460\n",
      "Epoch [112/1000], Loss: 0.1832\n",
      "Epoch [113/1000], Loss: 0.1807\n",
      "Epoch [114/1000], Loss: 0.1871\n",
      "Epoch [115/1000], Loss: 0.1711\n",
      "Epoch [116/1000], Loss: 0.2296\n",
      "Epoch [117/1000], Loss: 0.2145\n",
      "Epoch [118/1000], Loss: 0.1800\n",
      "Epoch [119/1000], Loss: 0.2083\n",
      "Epoch [120/1000], Loss: 0.1330\n",
      "Epoch [121/1000], Loss: 0.1609\n",
      "Epoch [122/1000], Loss: 0.1330\n",
      "Epoch [123/1000], Loss: 0.1563\n",
      "Epoch [124/1000], Loss: 0.1188\n",
      "Epoch [125/1000], Loss: 0.1630\n",
      "Epoch [126/1000], Loss: 0.1716\n",
      "Epoch [127/1000], Loss: 0.1692\n",
      "Epoch [128/1000], Loss: 0.1557\n",
      "Epoch [129/1000], Loss: 0.1656\n",
      "Epoch [130/1000], Loss: 0.1622\n",
      "Epoch [131/1000], Loss: 0.1302\n",
      "Epoch [132/1000], Loss: 0.1602\n",
      "Epoch [133/1000], Loss: 0.2031\n",
      "Epoch [134/1000], Loss: 0.1552\n",
      "Epoch [135/1000], Loss: 0.1501\n",
      "Epoch [136/1000], Loss: 0.1394\n",
      "Epoch [137/1000], Loss: 0.1325\n",
      "Epoch [138/1000], Loss: 0.1905\n",
      "Epoch [139/1000], Loss: 0.1444\n",
      "Epoch [140/1000], Loss: 0.1722\n",
      "Epoch [141/1000], Loss: 0.1584\n",
      "Epoch [142/1000], Loss: 0.1767\n",
      "Epoch [143/1000], Loss: 0.1371\n",
      "Epoch [144/1000], Loss: 0.1576\n",
      "Epoch [145/1000], Loss: 0.1635\n",
      "Epoch [146/1000], Loss: 0.1356\n",
      "Epoch [147/1000], Loss: 0.1926\n",
      "Epoch [148/1000], Loss: 0.1426\n",
      "Epoch [149/1000], Loss: 0.1500\n",
      "Epoch [150/1000], Loss: 0.1216\n",
      "Epoch [151/1000], Loss: 0.1519\n",
      "Epoch [152/1000], Loss: 0.1417\n",
      "Epoch [153/1000], Loss: 0.1032\n",
      "Epoch [154/1000], Loss: 0.2196\n",
      "Epoch [155/1000], Loss: 0.1086\n",
      "Epoch [156/1000], Loss: 0.1334\n",
      "Epoch [157/1000], Loss: 0.1003\n",
      "Epoch [158/1000], Loss: 0.1616\n",
      "Epoch [159/1000], Loss: 0.1367\n",
      "Epoch [160/1000], Loss: 0.1214\n",
      "Epoch [161/1000], Loss: 0.1378\n",
      "Epoch [162/1000], Loss: 0.1261\n",
      "Epoch [163/1000], Loss: 0.0961\n",
      "Epoch [164/1000], Loss: 0.1246\n",
      "Epoch [165/1000], Loss: 0.1373\n",
      "Epoch [166/1000], Loss: 0.1344\n",
      "Epoch [167/1000], Loss: 0.1031\n",
      "Epoch [168/1000], Loss: 0.1309\n",
      "Epoch [169/1000], Loss: 0.1029\n",
      "Epoch [170/1000], Loss: 0.1564\n",
      "Epoch [171/1000], Loss: 0.1134\n",
      "Epoch [172/1000], Loss: 0.0986\n",
      "Epoch [173/1000], Loss: 0.1332\n",
      "Epoch [174/1000], Loss: 0.1484\n",
      "Epoch [175/1000], Loss: 0.1088\n",
      "Epoch [176/1000], Loss: 0.1296\n",
      "Epoch [177/1000], Loss: 0.1421\n",
      "Epoch [178/1000], Loss: 0.1188\n",
      "Epoch [179/1000], Loss: 0.1056\n",
      "Epoch [180/1000], Loss: 0.1102\n",
      "Epoch [181/1000], Loss: 0.1409\n",
      "Epoch [182/1000], Loss: 0.1241\n",
      "Epoch [183/1000], Loss: 0.1275\n",
      "Epoch [184/1000], Loss: 0.1325\n",
      "Epoch [185/1000], Loss: 0.0791\n",
      "Epoch [186/1000], Loss: 0.0961\n",
      "Epoch [187/1000], Loss: 0.1130\n",
      "Epoch [188/1000], Loss: 0.0965\n",
      "Epoch [189/1000], Loss: 0.0937\n",
      "Epoch [190/1000], Loss: 0.1175\n",
      "Epoch [191/1000], Loss: 0.0882\n",
      "Epoch [192/1000], Loss: 0.1192\n",
      "Epoch [193/1000], Loss: 0.1211\n",
      "Epoch [194/1000], Loss: 0.1293\n",
      "Epoch [195/1000], Loss: 0.1110\n",
      "Epoch [196/1000], Loss: 0.1124\n",
      "Epoch [197/1000], Loss: 0.1253\n",
      "Epoch [198/1000], Loss: 0.0979\n",
      "Epoch [199/1000], Loss: 0.0975\n",
      "Epoch [200/1000], Loss: 0.0908\n",
      "Epoch [201/1000], Loss: 0.1236\n",
      "Epoch [202/1000], Loss: 0.1002\n",
      "Epoch [203/1000], Loss: 0.0921\n",
      "Epoch [204/1000], Loss: 0.0858\n",
      "Epoch [205/1000], Loss: 0.1138\n",
      "Epoch [206/1000], Loss: 0.0922\n",
      "Epoch [207/1000], Loss: 0.0784\n",
      "Epoch [208/1000], Loss: 0.0764\n",
      "Epoch [209/1000], Loss: 0.0915\n",
      "Epoch [210/1000], Loss: 0.1203\n",
      "Epoch [211/1000], Loss: 0.1066\n",
      "Epoch [212/1000], Loss: 0.0899\n",
      "Epoch [213/1000], Loss: 0.0976\n",
      "Epoch [214/1000], Loss: 0.1001\n",
      "Epoch [215/1000], Loss: 0.1036\n",
      "Epoch [216/1000], Loss: 0.0885\n",
      "Epoch [217/1000], Loss: 0.1060\n",
      "Epoch [218/1000], Loss: 0.0866\n",
      "Epoch [219/1000], Loss: 0.1116\n",
      "Epoch [220/1000], Loss: 0.0835\n",
      "Epoch [221/1000], Loss: 0.0996\n",
      "Epoch [222/1000], Loss: 0.0756\n",
      "Epoch [223/1000], Loss: 0.0908\n",
      "Epoch [224/1000], Loss: 0.0728\n",
      "Epoch [225/1000], Loss: 0.0952\n",
      "Epoch [226/1000], Loss: 0.0829\n",
      "Epoch [227/1000], Loss: 0.0941\n",
      "Epoch [228/1000], Loss: 0.0857\n",
      "Epoch [229/1000], Loss: 0.0577\n",
      "Epoch [230/1000], Loss: 0.0741\n",
      "Epoch [231/1000], Loss: 0.0797\n",
      "Epoch [232/1000], Loss: 0.0700\n",
      "Epoch [233/1000], Loss: 0.0763\n",
      "Epoch [234/1000], Loss: 0.0785\n",
      "Epoch [235/1000], Loss: 0.0823\n",
      "Epoch [236/1000], Loss: 0.0826\n",
      "Epoch [237/1000], Loss: 0.0969\n",
      "Epoch [238/1000], Loss: 0.0880\n",
      "Epoch [239/1000], Loss: 0.0844\n",
      "Epoch [240/1000], Loss: 0.0770\n",
      "Epoch [241/1000], Loss: 0.0700\n",
      "Epoch [242/1000], Loss: 0.0857\n",
      "Epoch [243/1000], Loss: 0.0773\n",
      "Epoch [244/1000], Loss: 0.0658\n",
      "Epoch [245/1000], Loss: 0.0737\n",
      "Epoch [246/1000], Loss: 0.0717\n",
      "Epoch [247/1000], Loss: 0.0675\n",
      "Epoch [248/1000], Loss: 0.0917\n",
      "Epoch [249/1000], Loss: 0.0807\n",
      "Epoch [250/1000], Loss: 0.0830\n",
      "Epoch [251/1000], Loss: 0.0658\n",
      "Epoch [252/1000], Loss: 0.0768\n",
      "Epoch [253/1000], Loss: 0.0701\n",
      "Epoch [254/1000], Loss: 0.0786\n",
      "Epoch [255/1000], Loss: 0.0558\n",
      "Epoch [256/1000], Loss: 0.0714\n",
      "Epoch [257/1000], Loss: 0.0656\n",
      "Epoch [258/1000], Loss: 0.0778\n",
      "Epoch [259/1000], Loss: 0.0582\n",
      "Epoch [260/1000], Loss: 0.0615\n",
      "Epoch [261/1000], Loss: 0.0627\n",
      "Epoch [262/1000], Loss: 0.0729\n",
      "Epoch [263/1000], Loss: 0.0823\n",
      "Epoch [264/1000], Loss: 0.0615\n",
      "Epoch [265/1000], Loss: 0.0726\n",
      "Epoch [266/1000], Loss: 0.0698\n",
      "Epoch [267/1000], Loss: 0.0736\n",
      "Epoch [268/1000], Loss: 0.0839\n",
      "Epoch [269/1000], Loss: 0.0931\n",
      "Epoch [270/1000], Loss: 0.0564\n",
      "Epoch [271/1000], Loss: 0.0690\n",
      "Epoch [272/1000], Loss: 0.0720\n",
      "Epoch [273/1000], Loss: 0.0792\n",
      "Epoch [274/1000], Loss: 0.0703\n",
      "Epoch [275/1000], Loss: 0.0782\n",
      "Epoch [276/1000], Loss: 0.0827\n",
      "Epoch [277/1000], Loss: 0.0645\n",
      "Epoch [278/1000], Loss: 0.0649\n",
      "Epoch [279/1000], Loss: 0.0514\n",
      "Epoch [280/1000], Loss: 0.0548\n",
      "Epoch [281/1000], Loss: 0.0635\n",
      "Epoch [282/1000], Loss: 0.0515\n",
      "Epoch [283/1000], Loss: 0.0530\n",
      "Epoch [284/1000], Loss: 0.0529\n",
      "Epoch [285/1000], Loss: 0.0573\n",
      "Epoch [286/1000], Loss: 0.0617\n",
      "Epoch [287/1000], Loss: 0.0635\n",
      "Epoch [288/1000], Loss: 0.0618\n",
      "Epoch [289/1000], Loss: 0.0538\n",
      "Epoch [290/1000], Loss: 0.0653\n",
      "Epoch [291/1000], Loss: 0.0620\n",
      "Epoch [292/1000], Loss: 0.0647\n",
      "Epoch [293/1000], Loss: 0.0473\n",
      "Epoch [294/1000], Loss: 0.0668\n",
      "Epoch [295/1000], Loss: 0.0598\n",
      "Epoch [296/1000], Loss: 0.0444\n",
      "Epoch [297/1000], Loss: 0.0554\n",
      "Epoch [298/1000], Loss: 0.0672\n",
      "Epoch [299/1000], Loss: 0.0624\n",
      "Epoch [300/1000], Loss: 0.0714\n",
      "Epoch [301/1000], Loss: 0.0523\n",
      "Epoch [302/1000], Loss: 0.0524\n",
      "Epoch [303/1000], Loss: 0.0512\n",
      "Epoch [304/1000], Loss: 0.0471\n",
      "Epoch [305/1000], Loss: 0.0592\n",
      "Epoch [306/1000], Loss: 0.0711\n",
      "Epoch [307/1000], Loss: 0.0448\n",
      "Epoch [308/1000], Loss: 0.0690\n",
      "Epoch [309/1000], Loss: 0.0522\n",
      "Epoch [310/1000], Loss: 0.0441\n",
      "Epoch [311/1000], Loss: 0.0427\n",
      "Epoch [312/1000], Loss: 0.0583\n",
      "Epoch [313/1000], Loss: 0.0596\n",
      "Epoch [314/1000], Loss: 0.0407\n",
      "Epoch [315/1000], Loss: 0.0496\n",
      "Epoch [316/1000], Loss: 0.0543\n",
      "Epoch [317/1000], Loss: 0.0505\n",
      "Epoch [318/1000], Loss: 0.0534\n",
      "Epoch [319/1000], Loss: 0.0538\n",
      "Epoch [320/1000], Loss: 0.0507\n",
      "Epoch [321/1000], Loss: 0.0613\n",
      "Epoch [322/1000], Loss: 0.0562\n",
      "Epoch [323/1000], Loss: 0.0534\n",
      "Epoch [324/1000], Loss: 0.0479\n",
      "Epoch [325/1000], Loss: 0.0400\n",
      "Epoch [326/1000], Loss: 0.0463\n",
      "Epoch [327/1000], Loss: 0.0592\n",
      "Epoch [328/1000], Loss: 0.0519\n",
      "Epoch [329/1000], Loss: 0.0470\n",
      "Epoch [330/1000], Loss: 0.0507\n",
      "Epoch [331/1000], Loss: 0.0454\n",
      "Epoch [332/1000], Loss: 0.0440\n",
      "Epoch [333/1000], Loss: 0.0489\n",
      "Epoch [334/1000], Loss: 0.0609\n",
      "Epoch [335/1000], Loss: 0.0513\n",
      "Epoch [336/1000], Loss: 0.0449\n",
      "Epoch [337/1000], Loss: 0.0462\n",
      "Epoch [338/1000], Loss: 0.0481\n",
      "Epoch [339/1000], Loss: 0.0505\n",
      "Epoch [340/1000], Loss: 0.0524\n",
      "Epoch [341/1000], Loss: 0.0565\n",
      "Epoch [342/1000], Loss: 0.0561\n",
      "Epoch [343/1000], Loss: 0.0415\n",
      "Epoch [344/1000], Loss: 0.0449\n",
      "Epoch [345/1000], Loss: 0.0456\n",
      "Epoch [346/1000], Loss: 0.0462\n",
      "Epoch [347/1000], Loss: 0.0451\n",
      "Epoch [348/1000], Loss: 0.0456\n",
      "Epoch [349/1000], Loss: 0.0427\n",
      "Epoch [350/1000], Loss: 0.0388\n",
      "Epoch [351/1000], Loss: 0.0516\n",
      "Epoch [352/1000], Loss: 0.0449\n",
      "Epoch [353/1000], Loss: 0.0391\n",
      "Epoch [354/1000], Loss: 0.0348\n",
      "Epoch [355/1000], Loss: 0.0388\n",
      "Epoch [356/1000], Loss: 0.0423\n",
      "Epoch [357/1000], Loss: 0.0344\n",
      "Epoch [358/1000], Loss: 0.0414\n",
      "Epoch [359/1000], Loss: 0.0377\n",
      "Epoch [360/1000], Loss: 0.0470\n",
      "Epoch [361/1000], Loss: 0.0423\n",
      "Epoch [362/1000], Loss: 0.0386\n",
      "Epoch [363/1000], Loss: 0.0467\n",
      "Epoch [364/1000], Loss: 0.0315\n",
      "Epoch [365/1000], Loss: 0.0364\n",
      "Epoch [366/1000], Loss: 0.0331\n",
      "Epoch [367/1000], Loss: 0.0377\n",
      "Epoch [368/1000], Loss: 0.0434\n",
      "Epoch [369/1000], Loss: 0.0519\n",
      "Epoch [370/1000], Loss: 0.0326\n",
      "Epoch [371/1000], Loss: 0.0400\n",
      "Epoch [372/1000], Loss: 0.0451\n",
      "Epoch [373/1000], Loss: 0.0369\n",
      "Epoch [374/1000], Loss: 0.0435\n",
      "Epoch [375/1000], Loss: 0.0422\n",
      "Epoch [376/1000], Loss: 0.0316\n",
      "Epoch [377/1000], Loss: 0.0364\n",
      "Epoch [378/1000], Loss: 0.0391\n",
      "Epoch [379/1000], Loss: 0.0316\n",
      "Epoch [380/1000], Loss: 0.0352\n",
      "Epoch [381/1000], Loss: 0.0412\n",
      "Epoch [382/1000], Loss: 0.0347\n",
      "Epoch [383/1000], Loss: 0.0372\n",
      "Epoch [384/1000], Loss: 0.0354\n",
      "Epoch [385/1000], Loss: 0.0363\n",
      "Epoch [386/1000], Loss: 0.0283\n",
      "Epoch [387/1000], Loss: 0.0312\n",
      "Epoch [388/1000], Loss: 0.0347\n",
      "Epoch [389/1000], Loss: 0.0343\n",
      "Epoch [390/1000], Loss: 0.0330\n",
      "Epoch [391/1000], Loss: 0.0428\n",
      "Epoch [392/1000], Loss: 0.0276\n",
      "Epoch [393/1000], Loss: 0.0365\n",
      "Epoch [394/1000], Loss: 0.0335\n",
      "Epoch [395/1000], Loss: 0.0410\n",
      "Epoch [396/1000], Loss: 0.0407\n",
      "Epoch [397/1000], Loss: 0.0321\n",
      "Epoch [398/1000], Loss: 0.0297\n",
      "Epoch [399/1000], Loss: 0.0360\n",
      "Epoch [400/1000], Loss: 0.0313\n",
      "Epoch [401/1000], Loss: 0.0338\n",
      "Epoch [402/1000], Loss: 0.0322\n",
      "Epoch [403/1000], Loss: 0.0331\n",
      "Epoch [404/1000], Loss: 0.0392\n",
      "Epoch [405/1000], Loss: 0.0398\n",
      "Epoch [406/1000], Loss: 0.0290\n",
      "Epoch [407/1000], Loss: 0.0372\n",
      "Epoch [408/1000], Loss: 0.0304\n",
      "Epoch [409/1000], Loss: 0.0286\n",
      "Epoch [410/1000], Loss: 0.0361\n",
      "Epoch [411/1000], Loss: 0.0324\n",
      "Epoch [412/1000], Loss: 0.0356\n",
      "Epoch [413/1000], Loss: 0.0241\n",
      "Epoch [414/1000], Loss: 0.0270\n",
      "Epoch [415/1000], Loss: 0.0305\n",
      "Epoch [416/1000], Loss: 0.0292\n",
      "Epoch [417/1000], Loss: 0.0299\n",
      "Epoch [418/1000], Loss: 0.0314\n",
      "Epoch [419/1000], Loss: 0.0438\n",
      "Epoch [420/1000], Loss: 0.0247\n",
      "Epoch [421/1000], Loss: 0.0277\n",
      "Epoch [422/1000], Loss: 0.0261\n",
      "Epoch [423/1000], Loss: 0.0345\n",
      "Epoch [424/1000], Loss: 0.0277\n",
      "Epoch [425/1000], Loss: 0.0284\n",
      "Epoch [426/1000], Loss: 0.0265\n",
      "Epoch [427/1000], Loss: 0.0336\n",
      "Epoch [428/1000], Loss: 0.0277\n",
      "Epoch [429/1000], Loss: 0.0291\n",
      "Epoch [430/1000], Loss: 0.0260\n",
      "Epoch [431/1000], Loss: 0.0280\n",
      "Epoch [432/1000], Loss: 0.0256\n",
      "Epoch [433/1000], Loss: 0.0254\n",
      "Epoch [434/1000], Loss: 0.0256\n",
      "Epoch [435/1000], Loss: 0.0300\n",
      "Epoch [436/1000], Loss: 0.0290\n",
      "Epoch [437/1000], Loss: 0.0303\n",
      "Epoch [438/1000], Loss: 0.0284\n",
      "Epoch [439/1000], Loss: 0.0339\n",
      "Epoch [440/1000], Loss: 0.0249\n",
      "Epoch [441/1000], Loss: 0.0264\n",
      "Epoch [442/1000], Loss: 0.0258\n",
      "Epoch [443/1000], Loss: 0.0226\n",
      "Epoch [444/1000], Loss: 0.0291\n",
      "Epoch [445/1000], Loss: 0.0254\n",
      "Epoch [446/1000], Loss: 0.0355\n",
      "Epoch [447/1000], Loss: 0.0222\n",
      "Epoch [448/1000], Loss: 0.0259\n",
      "Epoch [449/1000], Loss: 0.0224\n",
      "Epoch [450/1000], Loss: 0.0306\n",
      "Epoch [451/1000], Loss: 0.0340\n",
      "Epoch [452/1000], Loss: 0.0281\n",
      "Epoch [453/1000], Loss: 0.0260\n",
      "Epoch [454/1000], Loss: 0.0264\n",
      "Epoch [455/1000], Loss: 0.0225\n",
      "Epoch [456/1000], Loss: 0.0270\n",
      "Epoch [457/1000], Loss: 0.0266\n",
      "Epoch [458/1000], Loss: 0.0227\n",
      "Epoch [459/1000], Loss: 0.0298\n",
      "Epoch [460/1000], Loss: 0.0199\n",
      "Epoch [461/1000], Loss: 0.0276\n",
      "Epoch [462/1000], Loss: 0.0315\n",
      "Epoch [463/1000], Loss: 0.0255\n",
      "Epoch [464/1000], Loss: 0.0318\n",
      "Epoch [465/1000], Loss: 0.0254\n",
      "Epoch [466/1000], Loss: 0.0247\n",
      "Epoch [467/1000], Loss: 0.0244\n",
      "Epoch [468/1000], Loss: 0.0215\n",
      "Epoch [469/1000], Loss: 0.0251\n",
      "Epoch [470/1000], Loss: 0.0356\n",
      "Epoch [471/1000], Loss: 0.0229\n",
      "Epoch [472/1000], Loss: 0.0186\n",
      "Epoch [473/1000], Loss: 0.0232\n",
      "Epoch [474/1000], Loss: 0.0204\n",
      "Epoch [475/1000], Loss: 0.0217\n",
      "Epoch [476/1000], Loss: 0.0221\n",
      "Epoch [477/1000], Loss: 0.0226\n",
      "Epoch [478/1000], Loss: 0.0216\n",
      "Epoch [479/1000], Loss: 0.0214\n",
      "Epoch [480/1000], Loss: 0.0194\n",
      "Epoch [481/1000], Loss: 0.0248\n",
      "Epoch [482/1000], Loss: 0.0203\n",
      "Epoch [483/1000], Loss: 0.0217\n",
      "Epoch [484/1000], Loss: 0.0237\n",
      "Epoch [485/1000], Loss: 0.0216\n",
      "Epoch [486/1000], Loss: 0.0208\n",
      "Epoch [487/1000], Loss: 0.0211\n",
      "Epoch [488/1000], Loss: 0.0256\n",
      "Epoch [489/1000], Loss: 0.0230\n",
      "Epoch [490/1000], Loss: 0.0213\n",
      "Epoch [491/1000], Loss: 0.0209\n",
      "Epoch [492/1000], Loss: 0.0196\n",
      "Epoch [493/1000], Loss: 0.0222\n",
      "Epoch [494/1000], Loss: 0.0183\n",
      "Epoch [495/1000], Loss: 0.0200\n",
      "Epoch [496/1000], Loss: 0.0202\n",
      "Epoch [497/1000], Loss: 0.0303\n",
      "Epoch [498/1000], Loss: 0.0184\n",
      "Epoch [499/1000], Loss: 0.0208\n",
      "Epoch [500/1000], Loss: 0.0199\n",
      "Epoch [501/1000], Loss: 0.0205\n",
      "Epoch [502/1000], Loss: 0.0183\n",
      "Epoch [503/1000], Loss: 0.0241\n",
      "Epoch [504/1000], Loss: 0.0200\n",
      "Epoch [505/1000], Loss: 0.0221\n",
      "Epoch [506/1000], Loss: 0.0295\n",
      "Epoch [507/1000], Loss: 0.0234\n",
      "Epoch [508/1000], Loss: 0.0234\n",
      "Epoch [509/1000], Loss: 0.0196\n",
      "Epoch [510/1000], Loss: 0.0193\n",
      "Epoch [511/1000], Loss: 0.0197\n",
      "Epoch [512/1000], Loss: 0.0208\n",
      "Epoch [513/1000], Loss: 0.0189\n",
      "Epoch [514/1000], Loss: 0.0198\n",
      "Epoch [515/1000], Loss: 0.0192\n",
      "Epoch [516/1000], Loss: 0.0210\n",
      "Epoch [517/1000], Loss: 0.0284\n",
      "Epoch [518/1000], Loss: 0.0230\n",
      "Epoch [519/1000], Loss: 0.0203\n",
      "Epoch [520/1000], Loss: 0.0172\n",
      "Epoch [521/1000], Loss: 0.0191\n",
      "Epoch [522/1000], Loss: 0.0220\n",
      "Epoch [523/1000], Loss: 0.0173\n",
      "Epoch [524/1000], Loss: 0.0166\n",
      "Epoch [525/1000], Loss: 0.0210\n",
      "Epoch [526/1000], Loss: 0.0204\n",
      "Epoch [527/1000], Loss: 0.0213\n",
      "Epoch [528/1000], Loss: 0.0197\n",
      "Epoch [529/1000], Loss: 0.0211\n",
      "Epoch [530/1000], Loss: 0.0174\n",
      "Epoch [531/1000], Loss: 0.0183\n",
      "Epoch [532/1000], Loss: 0.0163\n",
      "Epoch [533/1000], Loss: 0.0183\n",
      "Epoch [534/1000], Loss: 0.0215\n",
      "Epoch [535/1000], Loss: 0.0192\n",
      "Epoch [536/1000], Loss: 0.0200\n",
      "Epoch [537/1000], Loss: 0.0155\n",
      "Epoch [538/1000], Loss: 0.0171\n",
      "Epoch [539/1000], Loss: 0.0193\n",
      "Epoch [540/1000], Loss: 0.0194\n",
      "Epoch [541/1000], Loss: 0.0195\n",
      "Epoch [542/1000], Loss: 0.0170\n",
      "Epoch [543/1000], Loss: 0.0201\n",
      "Epoch [544/1000], Loss: 0.0168\n",
      "Epoch [545/1000], Loss: 0.0182\n",
      "Epoch [546/1000], Loss: 0.0187\n",
      "Epoch [547/1000], Loss: 0.0177\n",
      "Epoch [548/1000], Loss: 0.0157\n",
      "Epoch [549/1000], Loss: 0.0176\n",
      "Epoch [550/1000], Loss: 0.0215\n",
      "Epoch [551/1000], Loss: 0.0163\n",
      "Epoch [552/1000], Loss: 0.0168\n",
      "Epoch [553/1000], Loss: 0.0160\n",
      "Epoch [554/1000], Loss: 0.0197\n",
      "Epoch [555/1000], Loss: 0.0130\n",
      "Epoch [556/1000], Loss: 0.0203\n",
      "Epoch [557/1000], Loss: 0.0175\n",
      "Epoch [558/1000], Loss: 0.0153\n",
      "Epoch [559/1000], Loss: 0.0225\n",
      "Epoch [560/1000], Loss: 0.0135\n",
      "Epoch [561/1000], Loss: 0.0177\n",
      "Epoch [562/1000], Loss: 0.0166\n",
      "Epoch [563/1000], Loss: 0.0184\n",
      "Epoch [564/1000], Loss: 0.0161\n",
      "Epoch [565/1000], Loss: 0.0143\n",
      "Epoch [566/1000], Loss: 0.0198\n",
      "Epoch [567/1000], Loss: 0.0171\n",
      "Epoch [568/1000], Loss: 0.0164\n",
      "Epoch [569/1000], Loss: 0.0174\n",
      "Epoch [570/1000], Loss: 0.0169\n",
      "Epoch [571/1000], Loss: 0.0177\n",
      "Epoch [572/1000], Loss: 0.0170\n",
      "Epoch [573/1000], Loss: 0.0155\n",
      "Epoch [574/1000], Loss: 0.0147\n",
      "Epoch [575/1000], Loss: 0.0165\n",
      "Epoch [576/1000], Loss: 0.0144\n",
      "Epoch [577/1000], Loss: 0.0149\n",
      "Epoch [578/1000], Loss: 0.0133\n",
      "Epoch [579/1000], Loss: 0.0137\n",
      "Epoch [580/1000], Loss: 0.0145\n",
      "Epoch [581/1000], Loss: 0.0143\n",
      "Epoch [582/1000], Loss: 0.0166\n",
      "Epoch [583/1000], Loss: 0.0148\n",
      "Epoch [584/1000], Loss: 0.0148\n",
      "Epoch [585/1000], Loss: 0.0189\n",
      "Epoch [586/1000], Loss: 0.0167\n",
      "Epoch [587/1000], Loss: 0.0139\n",
      "Epoch [588/1000], Loss: 0.0146\n",
      "Epoch [589/1000], Loss: 0.0156\n",
      "Epoch [590/1000], Loss: 0.0145\n",
      "Epoch [591/1000], Loss: 0.0147\n",
      "Epoch [592/1000], Loss: 0.0159\n",
      "Epoch [593/1000], Loss: 0.0142\n",
      "Epoch [594/1000], Loss: 0.0145\n",
      "Epoch [595/1000], Loss: 0.0142\n",
      "Epoch [596/1000], Loss: 0.0170\n",
      "Epoch [597/1000], Loss: 0.0181\n",
      "Epoch [598/1000], Loss: 0.0153\n",
      "Epoch [599/1000], Loss: 0.0130\n",
      "Epoch [600/1000], Loss: 0.0149\n",
      "Epoch [601/1000], Loss: 0.0132\n",
      "Epoch [602/1000], Loss: 0.0136\n",
      "Epoch [603/1000], Loss: 0.0154\n",
      "Epoch [604/1000], Loss: 0.0114\n",
      "Epoch [605/1000], Loss: 0.0138\n",
      "Epoch [606/1000], Loss: 0.0138\n",
      "Epoch [607/1000], Loss: 0.0156\n",
      "Epoch [608/1000], Loss: 0.0120\n",
      "Epoch [609/1000], Loss: 0.0123\n",
      "Epoch [610/1000], Loss: 0.0116\n",
      "Epoch [611/1000], Loss: 0.0139\n",
      "Epoch [612/1000], Loss: 0.0148\n",
      "Epoch [613/1000], Loss: 0.0186\n",
      "Epoch [614/1000], Loss: 0.0136\n",
      "Epoch [615/1000], Loss: 0.0121\n",
      "Epoch [616/1000], Loss: 0.0128\n",
      "Epoch [617/1000], Loss: 0.0119\n",
      "Epoch [618/1000], Loss: 0.0151\n",
      "Epoch [619/1000], Loss: 0.0126\n",
      "Epoch [620/1000], Loss: 0.0170\n",
      "Epoch [621/1000], Loss: 0.0104\n",
      "Epoch [622/1000], Loss: 0.0143\n",
      "Epoch [623/1000], Loss: 0.0136\n",
      "Epoch [624/1000], Loss: 0.0141\n",
      "Epoch [625/1000], Loss: 0.0142\n",
      "Epoch [626/1000], Loss: 0.0118\n",
      "Epoch [627/1000], Loss: 0.0147\n",
      "Epoch [628/1000], Loss: 0.0140\n",
      "Epoch [629/1000], Loss: 0.0157\n",
      "Epoch [630/1000], Loss: 0.0110\n",
      "Epoch [631/1000], Loss: 0.0131\n",
      "Epoch [632/1000], Loss: 0.0113\n",
      "Epoch [633/1000], Loss: 0.0112\n",
      "Epoch [634/1000], Loss: 0.0111\n",
      "Epoch [635/1000], Loss: 0.0139\n",
      "Epoch [636/1000], Loss: 0.0118\n",
      "Epoch [637/1000], Loss: 0.0135\n",
      "Epoch [638/1000], Loss: 0.0142\n",
      "Epoch [639/1000], Loss: 0.0154\n",
      "Epoch [640/1000], Loss: 0.0113\n",
      "Epoch [641/1000], Loss: 0.0134\n",
      "Epoch [642/1000], Loss: 0.0122\n",
      "Epoch [643/1000], Loss: 0.0109\n",
      "Epoch [644/1000], Loss: 0.0124\n",
      "Epoch [645/1000], Loss: 0.0118\n",
      "Epoch [646/1000], Loss: 0.0139\n",
      "Epoch [647/1000], Loss: 0.0121\n",
      "Epoch [648/1000], Loss: 0.0116\n",
      "Epoch [649/1000], Loss: 0.0153\n",
      "Epoch [650/1000], Loss: 0.0138\n",
      "Epoch [651/1000], Loss: 0.0116\n",
      "Epoch [652/1000], Loss: 0.0152\n",
      "Epoch [653/1000], Loss: 0.0117\n",
      "Epoch [654/1000], Loss: 0.0123\n",
      "Epoch [655/1000], Loss: 0.0104\n",
      "Epoch [656/1000], Loss: 0.0107\n",
      "Epoch [657/1000], Loss: 0.0123\n",
      "Epoch [658/1000], Loss: 0.0116\n",
      "Epoch [659/1000], Loss: 0.0149\n",
      "Epoch [660/1000], Loss: 0.0096\n",
      "Epoch [661/1000], Loss: 0.0116\n",
      "Epoch [662/1000], Loss: 0.0120\n",
      "Epoch [663/1000], Loss: 0.0116\n",
      "Epoch [664/1000], Loss: 0.0117\n",
      "Epoch [665/1000], Loss: 0.0109\n",
      "Epoch [666/1000], Loss: 0.0122\n",
      "Epoch [667/1000], Loss: 0.0106\n",
      "Epoch [668/1000], Loss: 0.0122\n",
      "Epoch [669/1000], Loss: 0.0108\n",
      "Epoch [670/1000], Loss: 0.0098\n",
      "Epoch [671/1000], Loss: 0.0117\n",
      "Epoch [672/1000], Loss: 0.0120\n",
      "Epoch [673/1000], Loss: 0.0108\n",
      "Epoch [674/1000], Loss: 0.0146\n",
      "Epoch [675/1000], Loss: 0.0118\n",
      "Epoch [676/1000], Loss: 0.0106\n",
      "Epoch [677/1000], Loss: 0.0093\n",
      "Epoch [678/1000], Loss: 0.0127\n",
      "Epoch [679/1000], Loss: 0.0096\n",
      "Epoch [680/1000], Loss: 0.0098\n",
      "Epoch [681/1000], Loss: 0.0119\n",
      "Epoch [682/1000], Loss: 0.0114\n",
      "Epoch [683/1000], Loss: 0.0112\n",
      "Epoch [684/1000], Loss: 0.0097\n",
      "Epoch [685/1000], Loss: 0.0109\n",
      "Epoch [686/1000], Loss: 0.0129\n",
      "Epoch [687/1000], Loss: 0.0141\n",
      "Epoch [688/1000], Loss: 0.0127\n",
      "Epoch [689/1000], Loss: 0.0120\n",
      "Epoch [690/1000], Loss: 0.0084\n",
      "Epoch [691/1000], Loss: 0.0092\n",
      "Epoch [692/1000], Loss: 0.0102\n",
      "Epoch [693/1000], Loss: 0.0118\n",
      "Epoch [694/1000], Loss: 0.0096\n",
      "Epoch [695/1000], Loss: 0.0099\n",
      "Epoch [696/1000], Loss: 0.0108\n",
      "Epoch [697/1000], Loss: 0.0128\n",
      "Epoch [698/1000], Loss: 0.0100\n",
      "Epoch [699/1000], Loss: 0.0090\n",
      "Epoch [700/1000], Loss: 0.0116\n",
      "Epoch [701/1000], Loss: 0.0107\n",
      "Epoch [702/1000], Loss: 0.0123\n",
      "Epoch [703/1000], Loss: 0.0097\n",
      "Epoch [704/1000], Loss: 0.0093\n",
      "Epoch [705/1000], Loss: 0.0103\n",
      "Epoch [706/1000], Loss: 0.0109\n",
      "Epoch [707/1000], Loss: 0.0112\n",
      "Epoch [708/1000], Loss: 0.0111\n",
      "Epoch [709/1000], Loss: 0.0100\n",
      "Epoch [710/1000], Loss: 0.0123\n",
      "Epoch [711/1000], Loss: 0.0103\n",
      "Epoch [712/1000], Loss: 0.0111\n",
      "Epoch [713/1000], Loss: 0.0125\n",
      "Epoch [714/1000], Loss: 0.0106\n",
      "Epoch [715/1000], Loss: 0.0105\n",
      "Epoch [716/1000], Loss: 0.0095\n",
      "Epoch [717/1000], Loss: 0.0100\n",
      "Epoch [718/1000], Loss: 0.0098\n",
      "Epoch [719/1000], Loss: 0.0093\n",
      "Epoch [720/1000], Loss: 0.0098\n",
      "Epoch [721/1000], Loss: 0.0090\n",
      "Epoch [722/1000], Loss: 0.0093\n",
      "Epoch [723/1000], Loss: 0.0107\n",
      "Epoch [724/1000], Loss: 0.0097\n",
      "Epoch [725/1000], Loss: 0.0103\n",
      "Epoch [726/1000], Loss: 0.0129\n",
      "Epoch [727/1000], Loss: 0.0090\n",
      "Epoch [728/1000], Loss: 0.0099\n",
      "Epoch [729/1000], Loss: 0.0117\n",
      "Epoch [730/1000], Loss: 0.0104\n",
      "Epoch [731/1000], Loss: 0.0094\n",
      "Epoch [732/1000], Loss: 0.0101\n",
      "Epoch [733/1000], Loss: 0.0092\n",
      "Epoch [734/1000], Loss: 0.0097\n",
      "Epoch [735/1000], Loss: 0.0111\n",
      "Epoch [736/1000], Loss: 0.0101\n",
      "Epoch [737/1000], Loss: 0.0100\n",
      "Epoch [738/1000], Loss: 0.0082\n",
      "Epoch [739/1000], Loss: 0.0080\n",
      "Epoch [740/1000], Loss: 0.0093\n",
      "Epoch [741/1000], Loss: 0.0086\n",
      "Epoch [742/1000], Loss: 0.0095\n",
      "Epoch [743/1000], Loss: 0.0096\n",
      "Epoch [744/1000], Loss: 0.0083\n",
      "Epoch [745/1000], Loss: 0.0117\n",
      "Epoch [746/1000], Loss: 0.0089\n",
      "Epoch [747/1000], Loss: 0.0075\n",
      "Epoch [748/1000], Loss: 0.0088\n",
      "Epoch [749/1000], Loss: 0.0115\n",
      "Epoch [750/1000], Loss: 0.0102\n",
      "Epoch [751/1000], Loss: 0.0107\n",
      "Epoch [752/1000], Loss: 0.0091\n",
      "Epoch [753/1000], Loss: 0.0080\n",
      "Epoch [754/1000], Loss: 0.0095\n",
      "Epoch [755/1000], Loss: 0.0085\n",
      "Epoch [756/1000], Loss: 0.0112\n",
      "Epoch [757/1000], Loss: 0.0110\n",
      "Epoch [758/1000], Loss: 0.0087\n",
      "Epoch [759/1000], Loss: 0.0089\n",
      "Epoch [760/1000], Loss: 0.0093\n",
      "Epoch [761/1000], Loss: 0.0104\n",
      "Epoch [762/1000], Loss: 0.0083\n",
      "Epoch [763/1000], Loss: 0.0094\n",
      "Epoch [764/1000], Loss: 0.0078\n",
      "Epoch [765/1000], Loss: 0.0095\n",
      "Epoch [766/1000], Loss: 0.0075\n",
      "Epoch [767/1000], Loss: 0.0079\n",
      "Epoch [768/1000], Loss: 0.0085\n",
      "Epoch [769/1000], Loss: 0.0092\n",
      "Epoch [770/1000], Loss: 0.0081\n",
      "Epoch [771/1000], Loss: 0.0079\n",
      "Epoch [772/1000], Loss: 0.0087\n",
      "Epoch [773/1000], Loss: 0.0086\n",
      "Epoch [774/1000], Loss: 0.0106\n",
      "Epoch [775/1000], Loss: 0.0077\n",
      "Epoch [776/1000], Loss: 0.0075\n",
      "Epoch [777/1000], Loss: 0.0090\n",
      "Epoch [778/1000], Loss: 0.0074\n",
      "Epoch [779/1000], Loss: 0.0100\n",
      "Epoch [780/1000], Loss: 0.0076\n",
      "Epoch [781/1000], Loss: 0.0086\n",
      "Epoch [782/1000], Loss: 0.0083\n",
      "Epoch [783/1000], Loss: 0.0104\n",
      "Epoch [784/1000], Loss: 0.0085\n",
      "Epoch [785/1000], Loss: 0.0087\n",
      "Epoch [786/1000], Loss: 0.0089\n",
      "Epoch [787/1000], Loss: 0.0098\n",
      "Epoch [788/1000], Loss: 0.0073\n",
      "Epoch [789/1000], Loss: 0.0094\n",
      "Epoch [790/1000], Loss: 0.0089\n",
      "Epoch [791/1000], Loss: 0.0075\n",
      "Epoch [792/1000], Loss: 0.0072\n",
      "Epoch [793/1000], Loss: 0.0073\n",
      "Epoch [794/1000], Loss: 0.0071\n",
      "Epoch [795/1000], Loss: 0.0077\n",
      "Epoch [796/1000], Loss: 0.0076\n",
      "Epoch [797/1000], Loss: 0.0082\n",
      "Epoch [798/1000], Loss: 0.0077\n",
      "Epoch [799/1000], Loss: 0.0073\n",
      "Epoch [800/1000], Loss: 0.0070\n",
      "Epoch [801/1000], Loss: 0.0086\n",
      "Epoch [802/1000], Loss: 0.0079\n",
      "Epoch [803/1000], Loss: 0.0066\n",
      "Epoch [804/1000], Loss: 0.0077\n",
      "Epoch [805/1000], Loss: 0.0086\n",
      "Epoch [806/1000], Loss: 0.0074\n",
      "Epoch [807/1000], Loss: 0.0083\n",
      "Epoch [808/1000], Loss: 0.0081\n",
      "Epoch [809/1000], Loss: 0.0078\n",
      "Epoch [810/1000], Loss: 0.0078\n",
      "Epoch [811/1000], Loss: 0.0085\n",
      "Epoch [812/1000], Loss: 0.0076\n",
      "Epoch [813/1000], Loss: 0.0071\n",
      "Epoch [814/1000], Loss: 0.0084\n",
      "Epoch [815/1000], Loss: 0.0065\n",
      "Epoch [816/1000], Loss: 0.0074\n",
      "Epoch [817/1000], Loss: 0.0092\n",
      "Epoch [818/1000], Loss: 0.0069\n",
      "Epoch [819/1000], Loss: 0.0086\n",
      "Epoch [820/1000], Loss: 0.0082\n",
      "Epoch [821/1000], Loss: 0.0079\n",
      "Epoch [822/1000], Loss: 0.0068\n",
      "Epoch [823/1000], Loss: 0.0066\n",
      "Epoch [824/1000], Loss: 0.0084\n",
      "Epoch [825/1000], Loss: 0.0083\n",
      "Epoch [826/1000], Loss: 0.0075\n",
      "Epoch [827/1000], Loss: 0.0092\n",
      "Epoch [828/1000], Loss: 0.0084\n",
      "Epoch [829/1000], Loss: 0.0077\n",
      "Epoch [830/1000], Loss: 0.0075\n",
      "Epoch [831/1000], Loss: 0.0076\n",
      "Epoch [832/1000], Loss: 0.0074\n",
      "Epoch [833/1000], Loss: 0.0077\n",
      "Epoch [834/1000], Loss: 0.0080\n",
      "Epoch [835/1000], Loss: 0.0076\n",
      "Epoch [836/1000], Loss: 0.0078\n",
      "Epoch [837/1000], Loss: 0.0072\n",
      "Epoch [838/1000], Loss: 0.0084\n",
      "Epoch [839/1000], Loss: 0.0077\n",
      "Epoch [840/1000], Loss: 0.0079\n",
      "Epoch [841/1000], Loss: 0.0078\n",
      "Epoch [842/1000], Loss: 0.0070\n",
      "Epoch [843/1000], Loss: 0.0085\n",
      "Epoch [844/1000], Loss: 0.0071\n",
      "Epoch [845/1000], Loss: 0.0065\n",
      "Epoch [846/1000], Loss: 0.0083\n",
      "Epoch [847/1000], Loss: 0.0079\n",
      "Epoch [848/1000], Loss: 0.0062\n",
      "Epoch [849/1000], Loss: 0.0070\n",
      "Epoch [850/1000], Loss: 0.0071\n",
      "Epoch [851/1000], Loss: 0.0064\n",
      "Epoch [852/1000], Loss: 0.0068\n",
      "Epoch [853/1000], Loss: 0.0066\n",
      "Epoch [854/1000], Loss: 0.0077\n",
      "Epoch [855/1000], Loss: 0.0063\n",
      "Epoch [856/1000], Loss: 0.0075\n",
      "Epoch [857/1000], Loss: 0.0079\n",
      "Epoch [858/1000], Loss: 0.0071\n",
      "Epoch [859/1000], Loss: 0.0085\n",
      "Epoch [860/1000], Loss: 0.0073\n",
      "Epoch [861/1000], Loss: 0.0082\n",
      "Epoch [862/1000], Loss: 0.0067\n",
      "Epoch [863/1000], Loss: 0.0091\n",
      "Epoch [864/1000], Loss: 0.0070\n",
      "Epoch [865/1000], Loss: 0.0061\n",
      "Epoch [866/1000], Loss: 0.0064\n",
      "Epoch [867/1000], Loss: 0.0066\n",
      "Epoch [868/1000], Loss: 0.0062\n",
      "Epoch [869/1000], Loss: 0.0056\n",
      "Epoch [870/1000], Loss: 0.0076\n",
      "Epoch [871/1000], Loss: 0.0079\n",
      "Epoch [872/1000], Loss: 0.0073\n",
      "Epoch [873/1000], Loss: 0.0078\n",
      "Epoch [874/1000], Loss: 0.0080\n",
      "Epoch [875/1000], Loss: 0.0066\n",
      "Epoch [876/1000], Loss: 0.0080\n",
      "Epoch [877/1000], Loss: 0.0061\n",
      "Epoch [878/1000], Loss: 0.0062\n",
      "Epoch [879/1000], Loss: 0.0065\n",
      "Epoch [880/1000], Loss: 0.0059\n",
      "Epoch [881/1000], Loss: 0.0082\n",
      "Epoch [882/1000], Loss: 0.0078\n",
      "Epoch [883/1000], Loss: 0.0085\n",
      "Epoch [884/1000], Loss: 0.0069\n",
      "Epoch [885/1000], Loss: 0.0077\n",
      "Epoch [886/1000], Loss: 0.0063\n",
      "Epoch [887/1000], Loss: 0.0067\n",
      "Epoch [888/1000], Loss: 0.0074\n",
      "Epoch [889/1000], Loss: 0.0059\n",
      "Epoch [890/1000], Loss: 0.0060\n",
      "Epoch [891/1000], Loss: 0.0057\n",
      "Epoch [892/1000], Loss: 0.0069\n",
      "Epoch [893/1000], Loss: 0.0075\n",
      "Epoch [894/1000], Loss: 0.0070\n",
      "Epoch [895/1000], Loss: 0.0065\n",
      "Epoch [896/1000], Loss: 0.0068\n",
      "Epoch [897/1000], Loss: 0.0059\n",
      "Epoch [898/1000], Loss: 0.0076\n",
      "Epoch [899/1000], Loss: 0.0066\n",
      "Epoch [900/1000], Loss: 0.0067\n",
      "Epoch [901/1000], Loss: 0.0075\n",
      "Epoch [902/1000], Loss: 0.0063\n",
      "Epoch [903/1000], Loss: 0.0066\n",
      "Epoch [904/1000], Loss: 0.0065\n",
      "Epoch [905/1000], Loss: 0.0065\n",
      "Epoch [906/1000], Loss: 0.0059\n",
      "Epoch [907/1000], Loss: 0.0067\n",
      "Epoch [908/1000], Loss: 0.0056\n",
      "Epoch [909/1000], Loss: 0.0056\n",
      "Epoch [910/1000], Loss: 0.0067\n",
      "Epoch [911/1000], Loss: 0.0062\n",
      "Epoch [912/1000], Loss: 0.0058\n",
      "Epoch [913/1000], Loss: 0.0064\n",
      "Epoch [914/1000], Loss: 0.0067\n",
      "Epoch [915/1000], Loss: 0.0062\n",
      "Epoch [916/1000], Loss: 0.0067\n",
      "Epoch [917/1000], Loss: 0.0054\n",
      "Epoch [918/1000], Loss: 0.0057\n",
      "Epoch [919/1000], Loss: 0.0057\n",
      "Epoch [920/1000], Loss: 0.0082\n",
      "Epoch [921/1000], Loss: 0.0075\n",
      "Epoch [922/1000], Loss: 0.0068\n",
      "Epoch [923/1000], Loss: 0.0061\n",
      "Epoch [924/1000], Loss: 0.0056\n",
      "Epoch [925/1000], Loss: 0.0053\n",
      "Epoch [926/1000], Loss: 0.0065\n",
      "Epoch [927/1000], Loss: 0.0058\n",
      "Epoch [928/1000], Loss: 0.0062\n",
      "Epoch [929/1000], Loss: 0.0058\n",
      "Epoch [930/1000], Loss: 0.0062\n",
      "Epoch [931/1000], Loss: 0.0058\n",
      "Epoch [932/1000], Loss: 0.0065\n",
      "Epoch [933/1000], Loss: 0.0061\n",
      "Epoch [934/1000], Loss: 0.0068\n",
      "Epoch [935/1000], Loss: 0.0068\n",
      "Epoch [936/1000], Loss: 0.0066\n",
      "Epoch [937/1000], Loss: 0.0069\n",
      "Epoch [938/1000], Loss: 0.0068\n",
      "Epoch [939/1000], Loss: 0.0062\n",
      "Epoch [940/1000], Loss: 0.0052\n",
      "Epoch [941/1000], Loss: 0.0069\n",
      "Epoch [942/1000], Loss: 0.0063\n",
      "Epoch [943/1000], Loss: 0.0066\n",
      "Epoch [944/1000], Loss: 0.0065\n",
      "Epoch [945/1000], Loss: 0.0059\n",
      "Epoch [946/1000], Loss: 0.0050\n",
      "Epoch [947/1000], Loss: 0.0061\n",
      "Epoch [948/1000], Loss: 0.0072\n",
      "Epoch [949/1000], Loss: 0.0074\n",
      "Epoch [950/1000], Loss: 0.0068\n",
      "Epoch [951/1000], Loss: 0.0053\n",
      "Epoch [952/1000], Loss: 0.0059\n",
      "Epoch [953/1000], Loss: 0.0077\n",
      "Epoch [954/1000], Loss: 0.0053\n",
      "Epoch [955/1000], Loss: 0.0058\n",
      "Epoch [956/1000], Loss: 0.0059\n",
      "Epoch [957/1000], Loss: 0.0048\n",
      "Epoch [958/1000], Loss: 0.0055\n",
      "Epoch [959/1000], Loss: 0.0053\n",
      "Epoch [960/1000], Loss: 0.0061\n",
      "Epoch [961/1000], Loss: 0.0060\n",
      "Epoch [962/1000], Loss: 0.0064\n",
      "Epoch [963/1000], Loss: 0.0069\n",
      "Epoch [964/1000], Loss: 0.0055\n",
      "Epoch [965/1000], Loss: 0.0071\n",
      "Epoch [966/1000], Loss: 0.0056\n",
      "Epoch [967/1000], Loss: 0.0052\n",
      "Epoch [968/1000], Loss: 0.0053\n",
      "Epoch [969/1000], Loss: 0.0059\n",
      "Epoch [970/1000], Loss: 0.0065\n",
      "Epoch [971/1000], Loss: 0.0060\n",
      "Epoch [972/1000], Loss: 0.0061\n",
      "Epoch [973/1000], Loss: 0.0066\n",
      "Epoch [974/1000], Loss: 0.0053\n",
      "Epoch [975/1000], Loss: 0.0055\n",
      "Epoch [976/1000], Loss: 0.0059\n",
      "Epoch [977/1000], Loss: 0.0053\n",
      "Epoch [978/1000], Loss: 0.0062\n",
      "Epoch [979/1000], Loss: 0.0057\n",
      "Epoch [980/1000], Loss: 0.0064\n",
      "Epoch [981/1000], Loss: 0.0061\n",
      "Epoch [982/1000], Loss: 0.0052\n",
      "Epoch [983/1000], Loss: 0.0060\n",
      "Epoch [984/1000], Loss: 0.0071\n",
      "Epoch [985/1000], Loss: 0.0053\n",
      "Epoch [986/1000], Loss: 0.0058\n",
      "Epoch [987/1000], Loss: 0.0061\n",
      "Epoch [988/1000], Loss: 0.0050\n",
      "Epoch [989/1000], Loss: 0.0061\n",
      "Epoch [990/1000], Loss: 0.0057\n",
      "Epoch [991/1000], Loss: 0.0055\n",
      "Epoch [992/1000], Loss: 0.0056\n",
      "Epoch [993/1000], Loss: 0.0053\n",
      "Epoch [994/1000], Loss: 0.0063\n",
      "Epoch [995/1000], Loss: 0.0057\n",
      "Epoch [996/1000], Loss: 0.0055\n",
      "Epoch [997/1000], Loss: 0.0067\n",
      "Epoch [998/1000], Loss: 0.0057\n",
      "Epoch [999/1000], Loss: 0.0059\n",
      "Epoch [1000/1000], Loss: 0.0061\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 初始化模型\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# 训练循环\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        seq1, seq2, vec1, vec2, labels = batch[\"seq1\"], batch[\"seq2\"], batch[\"vec1\"], batch[\"vec2\"], batch[\"label\"]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq1.to(\"cuda\"), seq2.to(\"cuda\"), vec1.to(\"cuda\"), vec2.to(\"cuda\"))\n",
    "        loss = loss_fn(output, labels.to(\"cuda\"))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa379870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch [1000/1000], Loss: 0.0061 ，\n",
    "# 用之前定义的 MSE（均方误差）来衡量模型输出和真实 Toric 特征的差距。越小说明预测越准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9835b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把训练好的权重存下来\n",
    "import torch\n",
    "torch.save(model.state_dict(), \"model_no_att.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b5077af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FusionModel() # 先重新创建一个相同结构的网络\n",
    "model.load_state_dict(torch.load(\"model_no_att.pth\", weights_only=True))# 把磁盘上的权重加载进来\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40fbc52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([138, 3, 121])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c20d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#预先在训练集上计算好的各类特征的均值和标准差从磁盘上读进来，准备用于反标准化，恢复到真实物理量\n",
    "Mean = torch.tensor(np.load(\"normalization/Mean.npy\")).to(\"cpu\") #动作特征均值\n",
    "Std = torch.tensor(np.load(\"normalization/Std.npy\")).to(\"cpu\") #标准差\n",
    "D_mean = torch.tensor(np.load(\"normalization/D_Mean.npy\")).to(\"cpu\") #根节点平移向量 D 的均值\n",
    "D_std = torch.tensor(np.load(\"normalization/D_Std.npy\")).to(\"cpu\")\n",
    "C_mean = torch.tensor(np.load(\"normalization/C_Mean.npy\")).to(\"cpu\") #Toric 相机参数 C 的均值\n",
    "C_std = torch.tensor(np.load(\"normalization/C_Std.npy\")).to(\"cpu\") #标准差\n",
    "\n",
    "#拆分\n",
    "x1, x2, x3 = torch.chunk(test_tensor[-1], chunks=3, dim=1)\n",
    "x1 = x1.permute(1,2,0)[0]\n",
    "D1 = x1[0][:9].to(torch.float32)\n",
    "motion1 = x1[1:].reshape(120,23,6).to(torch.float32)\n",
    "\n",
    "x2 = x2.permute(1,2,0)[0]\n",
    "D2 = x2[0][:9].to(torch.float32)\n",
    "motion2 = x2[1:].reshape(120,23,6).to(torch.float32)\n",
    "\n",
    "cam = model(motion1.to(\"cuda\").unsqueeze(0), motion2.to(\"cuda\").unsqueeze(0), \n",
    "            D1.to(\"cuda\").unsqueeze(0), D2.to(\"cuda\").unsqueeze(0))[0].to(torch.float32).to(\"cpu\")\n",
    "\n",
    "# 反标准化Denormalize，还原回真实坐标\n",
    "motion1 = motion1*(Std)+Mean\n",
    "motion2 = motion2*(Std)+Mean\n",
    "\n",
    "D1 = D1*(D_std)+D_mean\n",
    "D2 = D2*(D_std)+D_mean\n",
    "\n",
    "cam = cam*(C_std)+C_mean\n",
    "\n",
    "#保存结果到 .npy，准备可视化\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p0.npy\",motion1)\n",
    "np.save(\"../Camera/generated_data/new_joint_vecs/train/0_p1.npy\",motion2)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p0.npy\",D1)\n",
    "np.save(\"../Camera/generated_data/canon_data/train/0_p1.npy\",D2)\n",
    "np.save(\"../Camera/generated_data/camera/0.npy\",cam.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96736737-f214-4775-8850-ab09b2e6715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化脚本把这些.npy文件绘出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "095b8a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3077,  0.3124,  0.2330,  0.3106,  3.0105,  0.0698],\n",
       "        [-0.3094,  0.3102,  0.2323,  0.3084,  3.0063,  0.0668],\n",
       "        [-0.3050,  0.3094,  0.2400,  0.3070,  2.9929,  0.0667],\n",
       "        [-0.3014,  0.3094,  0.2361,  0.3056,  2.9873,  0.0769],\n",
       "        [-0.3105,  0.3070,  0.2413,  0.3107,  2.9589,  0.0649],\n",
       "        [-0.3171,  0.3017,  0.2473,  0.3131,  2.9468,  0.0726],\n",
       "        [-0.3113,  0.2963,  0.2483,  0.3011,  2.9268,  0.0617],\n",
       "        [-0.3170,  0.2909,  0.2339,  0.2991,  2.9536,  0.0646],\n",
       "        [-0.3159,  0.2900,  0.2456,  0.2957,  2.9546,  0.0621],\n",
       "        [-0.3106,  0.2893,  0.2369,  0.3016,  2.9262,  0.0580],\n",
       "        [-0.3085,  0.2942,  0.2412,  0.3118,  2.9338,  0.0593],\n",
       "        [-0.3108,  0.2880,  0.2477,  0.3134,  2.9290,  0.0602],\n",
       "        [-0.3114,  0.2917,  0.2528,  0.3033,  2.9059,  0.0547],\n",
       "        [-0.3128,  0.2901,  0.2501,  0.3032,  2.9317,  0.0485],\n",
       "        [-0.3167,  0.2854,  0.2488,  0.3063,  2.9056,  0.0533],\n",
       "        [-0.3071,  0.2970,  0.2439,  0.3058,  2.9371,  0.0596],\n",
       "        [-0.3129,  0.2981,  0.2370,  0.3054,  2.9412,  0.0519],\n",
       "        [-0.3166,  0.2960,  0.2420,  0.3019,  2.9056,  0.0557],\n",
       "        [-0.3121,  0.2964,  0.2541,  0.3076,  2.9145,  0.0530],\n",
       "        [-0.3224,  0.2912,  0.2468,  0.3042,  2.9342,  0.0551],\n",
       "        [-0.3219,  0.2938,  0.2361,  0.3097,  2.9618,  0.0518],\n",
       "        [-0.3269,  0.2945,  0.2361,  0.3074,  2.9826,  0.0497],\n",
       "        [-0.3217,  0.2862,  0.2419,  0.3071,  2.9952,  0.0421],\n",
       "        [-0.3269,  0.2952,  0.2341,  0.3018,  2.9831,  0.0520],\n",
       "        [-0.3133,  0.2925,  0.2352,  0.3009,  3.0011,  0.0434],\n",
       "        [-0.3206,  0.2992,  0.2385,  0.3107,  2.9667,  0.0543],\n",
       "        [-0.3049,  0.3034,  0.2370,  0.3064,  2.9753,  0.0509],\n",
       "        [-0.3114,  0.3022,  0.2288,  0.3072,  2.9325,  0.0480],\n",
       "        [-0.3063,  0.3052,  0.2347,  0.3077,  2.9554,  0.0429],\n",
       "        [-0.3124,  0.3050,  0.2329,  0.3113,  2.9674,  0.0463],\n",
       "        [-0.3096,  0.3086,  0.2338,  0.3133,  2.9658,  0.0490],\n",
       "        [-0.3119,  0.3031,  0.2331,  0.3081,  2.9634,  0.0473],\n",
       "        [-0.3099,  0.3015,  0.2284,  0.3107,  3.0033,  0.0426],\n",
       "        [-0.3139,  0.3040,  0.2230,  0.3150,  2.9559,  0.0366],\n",
       "        [-0.3115,  0.3080,  0.2311,  0.3129,  2.9794,  0.0441],\n",
       "        [-0.3151,  0.2984,  0.2288,  0.3199,  2.9870,  0.0363],\n",
       "        [-0.3006,  0.3062,  0.2318,  0.3145,  2.9800,  0.0373],\n",
       "        [-0.3059,  0.3040,  0.2354,  0.3142,  2.9629,  0.0346],\n",
       "        [-0.3094,  0.3050,  0.2362,  0.3226,  2.9966,  0.0343],\n",
       "        [-0.3116,  0.3073,  0.2312,  0.3217,  2.9877,  0.0411],\n",
       "        [-0.3009,  0.2996,  0.2339,  0.3163,  3.0103,  0.0315],\n",
       "        [-0.3006,  0.3042,  0.2349,  0.3194,  2.9553,  0.0348],\n",
       "        [-0.3063,  0.3098,  0.2269,  0.3145,  2.9383,  0.0332],\n",
       "        [-0.2988,  0.3015,  0.2349,  0.3127,  2.9890,  0.0430],\n",
       "        [-0.3063,  0.3038,  0.2347,  0.3144,  2.9874,  0.0405],\n",
       "        [-0.3002,  0.3048,  0.2377,  0.3141,  2.9918,  0.0406],\n",
       "        [-0.3012,  0.2960,  0.2373,  0.3158,  3.0068,  0.0374],\n",
       "        [-0.2987,  0.3011,  0.2366,  0.3175,  2.9921,  0.0426],\n",
       "        [-0.2914,  0.3015,  0.2358,  0.3144,  2.9971,  0.0427],\n",
       "        [-0.2987,  0.3014,  0.2352,  0.3144,  2.9945,  0.0503],\n",
       "        [-0.2958,  0.2987,  0.2380,  0.3136,  2.9917,  0.0573],\n",
       "        [-0.2970,  0.3060,  0.2323,  0.3174,  2.9765,  0.0597],\n",
       "        [-0.2915,  0.3020,  0.2355,  0.3116,  3.0193,  0.0509],\n",
       "        [-0.2857,  0.3068,  0.2351,  0.3105,  2.9798,  0.0624],\n",
       "        [-0.2834,  0.3024,  0.2318,  0.3096,  3.0091,  0.0664],\n",
       "        [-0.2856,  0.2992,  0.2341,  0.3133,  3.0243,  0.0626],\n",
       "        [-0.2841,  0.3027,  0.2310,  0.3138,  3.0092,  0.0607],\n",
       "        [-0.2835,  0.2987,  0.2336,  0.3204,  3.0492,  0.0643],\n",
       "        [-0.2843,  0.2972,  0.2365,  0.3168,  3.0167,  0.0593],\n",
       "        [-0.2817,  0.3032,  0.2339,  0.3182,  2.9911,  0.0592],\n",
       "        [-0.2910,  0.2996,  0.2332,  0.3144,  2.9819,  0.0602],\n",
       "        [-0.2874,  0.2951,  0.2276,  0.3155,  3.0108,  0.0616],\n",
       "        [-0.2845,  0.3017,  0.2350,  0.3141,  3.0212,  0.0639],\n",
       "        [-0.2897,  0.2981,  0.2331,  0.3166,  3.0207,  0.0654],\n",
       "        [-0.2821,  0.2959,  0.2330,  0.3141,  3.0056,  0.0681],\n",
       "        [-0.2821,  0.2964,  0.2323,  0.3117,  3.0052,  0.0598],\n",
       "        [-0.2829,  0.3079,  0.2309,  0.3163,  3.0123,  0.0645],\n",
       "        [-0.2855,  0.3030,  0.2261,  0.3139,  3.0155,  0.0641],\n",
       "        [-0.2857,  0.2995,  0.2266,  0.3173,  3.0278,  0.0540],\n",
       "        [-0.2839,  0.2961,  0.2365,  0.3175,  3.0442,  0.0516],\n",
       "        [-0.2794,  0.2903,  0.2247,  0.3206,  3.0225,  0.0570],\n",
       "        [-0.2869,  0.2967,  0.2279,  0.3201,  3.0173,  0.0609],\n",
       "        [-0.2858,  0.2924,  0.2281,  0.3215,  3.0298,  0.0546],\n",
       "        [-0.2846,  0.3030,  0.2297,  0.3167,  3.0051,  0.0667],\n",
       "        [-0.2872,  0.2895,  0.2299,  0.3169,  3.0247,  0.0586],\n",
       "        [-0.2747,  0.2983,  0.2330,  0.3174,  3.0152,  0.0672],\n",
       "        [-0.2836,  0.3025,  0.2282,  0.3155,  3.0087,  0.0646],\n",
       "        [-0.2843,  0.2967,  0.2234,  0.3137,  3.0093,  0.0672],\n",
       "        [-0.2904,  0.2994,  0.2283,  0.3137,  3.0150,  0.0706],\n",
       "        [-0.2924,  0.3051,  0.2208,  0.3180,  2.9668,  0.0730],\n",
       "        [-0.2903,  0.3023,  0.2252,  0.3142,  2.9679,  0.0686],\n",
       "        [-0.2870,  0.3021,  0.2237,  0.3140,  2.9796,  0.0659],\n",
       "        [-0.2938,  0.3031,  0.2214,  0.3066,  2.9656,  0.0720],\n",
       "        [-0.2871,  0.3036,  0.2263,  0.3068,  2.9227,  0.0723],\n",
       "        [-0.2841,  0.3025,  0.2315,  0.3060,  2.9509,  0.0747],\n",
       "        [-0.2820,  0.2999,  0.2299,  0.3026,  2.9125,  0.0698],\n",
       "        [-0.2806,  0.3053,  0.2367,  0.3037,  2.8891,  0.0721],\n",
       "        [-0.2828,  0.3045,  0.2332,  0.3045,  2.9082,  0.0655],\n",
       "        [-0.2752,  0.3095,  0.2339,  0.3073,  2.8745,  0.0658],\n",
       "        [-0.2850,  0.3062,  0.2330,  0.3013,  2.8961,  0.0626],\n",
       "        [-0.2838,  0.3167,  0.2283,  0.3027,  2.8653,  0.0685],\n",
       "        [-0.2778,  0.3231,  0.2279,  0.3013,  2.8553,  0.0679],\n",
       "        [-0.2745,  0.3193,  0.2257,  0.3031,  2.8735,  0.0745],\n",
       "        [-0.2677,  0.3260,  0.2340,  0.3014,  2.8383,  0.0745],\n",
       "        [-0.2755,  0.3196,  0.2386,  0.3065,  2.8683,  0.0718],\n",
       "        [-0.2659,  0.3202,  0.2366,  0.3034,  2.8372,  0.0735],\n",
       "        [-0.2619,  0.3236,  0.2339,  0.3019,  2.8508,  0.0730],\n",
       "        [-0.2619,  0.3241,  0.2335,  0.2999,  2.8406,  0.0685],\n",
       "        [-0.2652,  0.3227,  0.2370,  0.3054,  2.8165,  0.0647],\n",
       "        [-0.2703,  0.3244,  0.2351,  0.2991,  2.8446,  0.0727],\n",
       "        [-0.2701,  0.3295,  0.2378,  0.3066,  2.8171,  0.0724],\n",
       "        [-0.2734,  0.3281,  0.2361,  0.3065,  2.8417,  0.0753],\n",
       "        [-0.2698,  0.3301,  0.2376,  0.3075,  2.8348,  0.0657],\n",
       "        [-0.2719,  0.3287,  0.2411,  0.3109,  2.8393,  0.0743],\n",
       "        [-0.2754,  0.3247,  0.2407,  0.2984,  2.8466,  0.0737],\n",
       "        [-0.2763,  0.3291,  0.2372,  0.3039,  2.8753,  0.0740],\n",
       "        [-0.2747,  0.3276,  0.2390,  0.3011,  2.8468,  0.0759],\n",
       "        [-0.2752,  0.3199,  0.2426,  0.3064,  2.8733,  0.0713],\n",
       "        [-0.2762,  0.3208,  0.2455,  0.3023,  2.8535,  0.0802],\n",
       "        [-0.2678,  0.3229,  0.2441,  0.3145,  2.8426,  0.0731],\n",
       "        [-0.2726,  0.3240,  0.2399,  0.3094,  2.8514,  0.0708],\n",
       "        [-0.2694,  0.3250,  0.2425,  0.3081,  2.8720,  0.0715],\n",
       "        [-0.2661,  0.3248,  0.2430,  0.3123,  2.8553,  0.0735],\n",
       "        [-0.2736,  0.3192,  0.2374,  0.3121,  2.8413,  0.0654],\n",
       "        [-0.2629,  0.3266,  0.2465,  0.3049,  2.8285,  0.0730],\n",
       "        [-0.2649,  0.3181,  0.2415,  0.3126,  2.8475,  0.0728],\n",
       "        [-0.2672,  0.3189,  0.2434,  0.3129,  2.8876,  0.0661],\n",
       "        [-0.2575,  0.3191,  0.2410,  0.3116,  2.8649,  0.0745],\n",
       "        [-0.2560,  0.3218,  0.2435,  0.3126,  2.8610,  0.0759],\n",
       "        [-0.2674,  0.3226,  0.2452,  0.3143,  2.8747,  0.0811]],\n",
       "       dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71bf595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion1 and motion2 and D1 and D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef5f83a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 23, 6) (9,)\n",
      "0000:: torch.Size([1, 120, 23, 6])\n",
      "torch.Size([120, 22, 3])\n",
      "生成第0_p0个视频:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s5701147/MS/1/vis_generated_data.py\", line 191, in <module>\n",
      "    assert 1==2\n",
      "           ^^^^\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "! python vis_generated_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ff227-dada-4243-b348-da6edb4b913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step is Camera_vis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
